{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat US Census data (+ other datasets!)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import ast\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reformat_census_vars as refor\n",
    "import recs_EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/global/scratch/users/cristina_crespo/p1_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import census data for all CTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Num of census tract: https://www.census.gov/geographies/reference-files/time-series/geo/tallies.html\n",
    "\n",
    "United States: 84,414\t\n",
    "Puerto Rico: 981\t\n",
    "Island Areas: 133\t\n",
    "Total: 85,528\n",
    "\n",
    "CDD_TRACT has all of them - could check why we dont have 84k to start with !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_df = pd.read_csv(path +'in_us_census/acs5_2020_vbs_per_ct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make GEOID to create merges per CT\n",
    "census_df['state'] = census_df['state'].astype(str).str.zfill(2)\n",
    "census_df['county'] = census_df['county'].astype(str).str.zfill(3)\n",
    "census_df['tract'] = census_df['tract'].astype(str).str.zfill(6)\n",
    "\n",
    "census_df['GEOID'] = census_df[['state', 'county', 'tract']].astype(str).apply(lambda x: ''.join(x), axis=1)\n",
    "\n",
    "#delete hawaii, alaska, puerto rico\n",
    "census_df = census_df[~census_df.state.isin(['72'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(census_df.state.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have CTS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82907"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(census_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Add other datasets to re-create all variables in RECS model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Subtract water bodies from Census tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.usgs.gov/national-hydrography/access-national-hydrography-products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can onl get water bodies per state-county:\n",
    "https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2020&layergroup=Water\n",
    "https://www2.census.gov/geo/tiger/TIGER2023/\n",
    "--> Asked Chris Jones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Add CDD and HDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEMA historical CDD nad HDD CT data\n",
    "cdd_tract_raw = pd.read_csv(path +\"in_other_datasets/CDD_HDD/CDDbytract.csv\", encoding='utf-8',thousands=',').dropna()\n",
    "hdd_tract_raw = pd.read_csv(path +\"in_other_datasets/CDD_HDD/HDDbytract.csv\", encoding='utf-8',thousands=',').dropna()\n",
    "\n",
    "#rename cdd, hdd\n",
    "cdd_tract = cdd_tract_raw[['GEOID','hist']]\n",
    "cdd_tract.rename(columns = {'hist':'CDD'}, inplace = True)\n",
    "hdd_tract = hdd_tract_raw[['GEOID','hist']]\n",
    "hdd_tract.rename(columns = {'hist':'HDD'}, inplace = True)\n",
    "\n",
    "#get geoid in strig format\n",
    "cdd_tract['GEOID'] = cdd_tract['GEOID'].astype(str).str.zfill(11)\n",
    "hdd_tract['GEOID'] = hdd_tract['GEOID'].astype(str).str.zfill(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82907"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge cdd, hedd with census data\n",
    "enhanced_df  = census_df.merge(cdd_tract, on = 'GEOID', how = 'inner')\n",
    "enhanced_df  = enhanced_df.merge(hdd_tract, on = 'GEOID', how = 'inner')\n",
    "\n",
    "len(enhanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with 2010 census tracts. \n",
    "\n",
    "Select the census tract in 2010 that has the biggest area overalp with the 2020 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in original enhanced_df: 82907\n",
      "Rows in enhanced_df after crosswalk: 82907\n"
     ]
    }
   ],
   "source": [
    "#read in crosswalk file for CTs\n",
    "crosswalk_df = pd.read_csv('tab20_tract20_tract10_natl.txt', sep='|')\n",
    "\n",
    "# Rename the GEOID column in enhanced_df to GEOID_20 for clarity\n",
    "enhanced_df.rename(columns={'GEOID': 'GEOID_20'}, inplace=True)\n",
    "\n",
    "# Select relevant columns from the crosswalk\n",
    "crosswalk_df = crosswalk_df[['GEOID_TRACT_20', 'GEOID_TRACT_10', 'AREALAND_PART']]\n",
    "\n",
    "# Make sure the columns are treated as strings\n",
    "enhanced_df['GEOID_20'] = enhanced_df['GEOID_20'].astype(str)\n",
    "crosswalk_df['GEOID_TRACT_20'] = crosswalk_df['GEOID_TRACT_20'].astype(str).str.zfill(11)\n",
    "crosswalk_df['GEOID_TRACT_10'] = crosswalk_df['GEOID_TRACT_10'].astype(str).str.zfill(11)\n",
    "\n",
    "# Sort by AREALAND_PART in descending order to get the tract with the largest overlap\n",
    "crosswalk_df = crosswalk_df.sort_values(by='AREALAND_PART', ascending=False)\n",
    "crosswalk_df = crosswalk_df.drop_duplicates(subset='GEOID_TRACT_20', keep='first')  # Keep largest area\n",
    "\n",
    "# Merge enhanced_df with the crosswalk\n",
    "enhanced_df = pd.merge(enhanced_df, crosswalk_df[['GEOID_TRACT_20', 'GEOID_TRACT_10']], how='left', left_on='GEOID_20', right_on='GEOID_TRACT_20')\n",
    "\n",
    "# Drop the extra column and rename the 2010 GEOID column\n",
    "enhanced_df.drop(columns=['GEOID_TRACT_20'], inplace=True)\n",
    "enhanced_df.rename(columns={'GEOID_TRACT_10': 'GEOID_10'}, inplace=True)\n",
    "\n",
    "# Verify the final result\n",
    "print(f\"Rows in original enhanced_df: {len(enhanced_df)}\")\n",
    "print(f\"Rows in enhanced_df after crosswalk: {len(enhanced_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched 2020 tracts: 0\n",
      "If none, correct crosswalk file used\n"
     ]
    }
   ],
   "source": [
    "# Check if all GEOIDs in enhanced_df exist in crosswalk_df\n",
    "unmatched_geoids = enhanced_df[~enhanced_df['GEOID_20'].isin(crosswalk_df['GEOID_TRACT_20'])]\n",
    "print(f\"Number of unmatched 2020 tracts: {len(unmatched_geoids)}\")\n",
    "print('If none, correct crosswalk file used')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Add energy costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with missing energy costs after initial merge: 49\n",
      "Number of rows still missing energy costs after applying county averages: 15\n",
      "Number of rows still missing energy costs after applying state averages: 0\n",
      "All missing energy costs have been filled successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load and clean energy cost data\n",
    "energy_costs_fuel_raw = pd.read_excel(path + \"in_other_datasets/energy_costs/energycosts3.xlsx\", thousands=',').dropna()\n",
    "\n",
    "# Convert FIP to string format and pad with leading zeros to 11 characters\n",
    "energy_costs_fuel_raw['FIP'] = energy_costs_fuel_raw['FIP'].astype(int).astype(str).str.zfill(11)\n",
    "\n",
    "# Rename and select relevant columns\n",
    "energy_costs_fuel_raw.rename(columns={'FIP': 'GEOID_10', 'ELEP': 'cost_EL', 'GASP': 'cost_NG', 'FULP': 'cost_OF'}, inplace=True)\n",
    "\n",
    "# Inflate energy costs to 2020 dollars, save\n",
    "energy_costs_fuel_raw[['cost_EL', 'cost_NG', 'cost_OF']] *= 1.0307\n",
    "energy_costs_fuel = energy_costs_fuel_raw[['GEOID_10', 'cost_EL', 'cost_NG', 'cost_OF']]\n",
    "\n",
    "# Merge energy cost data with enhanced_df using GEOID_2020\n",
    "enhanced_df = enhanced_df.merge(energy_costs_fuel[['GEOID_10', 'cost_EL', 'cost_NG', 'cost_OF']], \n",
    "                                left_on='GEOID_10', right_on='GEOID_10', how='left')\n",
    "\n",
    "# Check for rows with missing energy costs after merge\n",
    "missing_rows = enhanced_df['cost_EL'].isna().sum()\n",
    "print(f\"Number of rows with missing energy costs after initial merge: {missing_rows}\")\n",
    "\n",
    "### 1. Fill missing values with COUNTY averages\n",
    "if missing_rows > 0:\n",
    "    # Extract county-level GEOID (first 5 digits of GEOID)\n",
    "    enhanced_df['county_GEOID_20'] = enhanced_df['GEOID_20'].str[:5]\n",
    "\n",
    "    # Calculate county-level averages for missing costs\n",
    "    county_avg = enhanced_df.groupby('county_GEOID_20')[['cost_EL', 'cost_NG', 'cost_OF']].mean().reset_index()\n",
    "\n",
    "    # Fill missing values by mapping county-level averages to the rows with missing data\n",
    "    for col in ['cost_EL', 'cost_NG', 'cost_OF']:\n",
    "        enhanced_df[col] = enhanced_df.apply(\n",
    "            lambda row: county_avg.loc[county_avg['county_GEOID_20'] == row['county_GEOID_20'], col].values[0]\n",
    "            if pd.isna(row[col]) and row['county_GEOID_20'] in county_avg['county_GEOID_20'].values\n",
    "            else row[col], axis=1\n",
    "        )\n",
    "\n",
    "    \n",
    "# Check for rows still missing after filling county averages\n",
    "missing_rows_after_county = enhanced_df['cost_EL'].isna().sum()\n",
    "print(f\"Number of rows still missing energy costs after applying county averages: {missing_rows_after_county}\")\n",
    "\n",
    "### 2. Fill remaining missing values with STATE averages\n",
    "if missing_rows_after_county > 0:\n",
    "    # Extract state-level GEOID (first 2 digits of GEOID)\n",
    "    enhanced_df['state_GEOID'] = enhanced_df['GEOID_20'].str[:2]\n",
    "\n",
    "    # Calculate state-level averages for missing costs\n",
    "    state_avg = enhanced_df.groupby('state_GEOID')[['cost_EL', 'cost_NG', 'cost_OF']].mean().reset_index()\n",
    "\n",
    "    # Fill remaining missing values with state-level averages\n",
    "    for col in ['cost_EL', 'cost_NG', 'cost_OF']:\n",
    "        enhanced_df[col] = enhanced_df.apply(\n",
    "            lambda row: state_avg.loc[state_avg['state_GEOID'] == row['state_GEOID'], col].values[0]\n",
    "            if pd.isna(row[col]) and row['state_GEOID'] in state_avg['state_GEOID'].values\n",
    "            else row[col], axis=1\n",
    "        )\n",
    "\n",
    "# Check for rows still missing after filling state averages\n",
    "final_missing_rows = enhanced_df['cost_EL'].isna().sum()\n",
    "print(f\"Number of rows still missing energy costs after applying state averages: {final_missing_rows}\")\n",
    "\n",
    "# Final check and summary\n",
    "if final_missing_rows > 0:\n",
    "    print(f\"Warning: {final_missing_rows} rows still missing energy costs after all filling attempts.\")\n",
    "else:\n",
    "    print(\"All missing energy costs have been filled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched 2010 tracts: 49\n"
     ]
    }
   ],
   "source": [
    "# Check if all GEOIDs in enhanced_df exist in energy_costs_fuel\n",
    "unmatched_geoids = enhanced_df[~enhanced_df['GEOID_10'].isin(energy_costs_fuel['GEOID_10'])]\n",
    "print(f\"Number of unmatched 2010 tracts: {len(unmatched_geoids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82907"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enhanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Add County/hh member AMI poverty levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows missing AMI data after county-level merge: 6\n",
      "Number of rows filled using state-level averages: 6\n",
      "Row count matches: 82907 rows in the final dataframe.\n",
      " Final row count remains consistent: 82907 rows.\n"
     ]
    }
   ],
   "source": [
    "# Load AMI (Area Median Income) data per county using 2010 GEOIDs and drop missing values\n",
    "ami_county_raw = pd.read_excel(path + \"in_other_datasets/AMI_county_hh_members/FY2020NSP_IncomeLimits.xlsx\").dropna()\n",
    "\n",
    "# Filter out states not included in the analysis (e.g., Puerto Rico - 72)\n",
    "excluded_states = ['66', '72', '78']\n",
    "ami_county_raw = ami_county_raw[~ami_county_raw['State'].isin(excluded_states)]\n",
    "\n",
    "# Create county-level GEOID using state and county codes\n",
    "ami_county_raw['State'] = ami_county_raw['State'].astype(str).str.zfill(2)\n",
    "ami_county_raw['County'] = ami_county_raw['County'].astype(str).str.zfill(3)\n",
    "ami_county_raw['county_GEOID_10'] = ami_county_raw['State'] + ami_county_raw['County']\n",
    "\n",
    "# Select relevant columns (GEOID_county and AMI limits for different household sizes)\n",
    "ami_county = ami_county_raw[['county_GEOID_10', 'lim120_20p1', 'lim120_20p2', 'lim120_20p3',\n",
    "                             'lim120_20p4', 'lim120_20p5', 'lim120_20p6', 'lim120_20p7', \n",
    "                             'lim120_20p8']]\n",
    "\n",
    "# Aggregate to county-level by taking the mean where necessary\n",
    "ami_county = ami_county.groupby('county_GEOID_10').mean().reset_index()\n",
    "\n",
    "# Extract 2010 county GEOIDs from enhanced_df for merging\n",
    "enhanced_df['county_GEOID_10'] = enhanced_df['GEOID_10'].str[:5]\n",
    "\n",
    "# Merge enhanced_df with county-level AMI data\n",
    "enhanced_df = enhanced_df.merge(ami_county, left_on='county_GEOID_10', right_on='county_GEOID_10', how='left')\n",
    "\n",
    "# Check how many rows have missing AMI data after county-level merge\n",
    "missing_rows_county = enhanced_df['lim120_20p1'].isna().sum()\n",
    "print(f\"Number of rows missing AMI data after county-level merge: {missing_rows_county}\")\n",
    "\n",
    "### 1. Fill missing values with STATE-level averages if county-level data is not available\n",
    "\n",
    "if missing_rows_county > 0:\n",
    "    \n",
    "    # Calculate state-level averages for missing AMI values\n",
    "    state_avg = ami_county_raw.copy()\n",
    "    state_avg['state_GEOID'] = state_avg['county_GEOID_10'].str[:2]\n",
    "    state_avg = state_avg.groupby('state_GEOID')[['lim120_20p1', 'lim120_20p2', 'lim120_20p3',\n",
    "                                                  'lim120_20p4', 'lim120_20p5', 'lim120_20p6', \n",
    "                                                  'lim120_20p7', 'lim120_20p8']].mean().reset_index()\n",
    "\n",
    "    # Fill missing rows using state-level averages\n",
    "    for col in ['lim120_20p1', 'lim120_20p2', 'lim120_20p3', 'lim120_20p4', \n",
    "                'lim120_20p5', 'lim120_20p6', 'lim120_20p7', 'lim120_20p8']:\n",
    "        enhanced_df[col] = enhanced_df.apply(\n",
    "            lambda row: state_avg.loc[state_avg['state_GEOID'] == row['state_GEOID'], col].values[0]\n",
    "            if pd.isna(row[col]) and row['state_GEOID'] in state_avg['state_GEOID'].values\n",
    "            else row[col], axis=1\n",
    "        )\n",
    "\n",
    "    # Check how many rows have been filled using state-level averages\n",
    "    missing_rows_state = enhanced_df['lim120_20p1'].isna().sum()\n",
    "    rows_filled_by_state = missing_rows_county - missing_rows_state\n",
    "    print(f\"Number of rows filled using state-level averages: {rows_filled_by_state}\")\n",
    "\n",
    "# Double-check that the final row count matches the original row count\n",
    "final_row_count = len(enhanced_df)\n",
    "initial_row_count = len(enhanced_df['GEOID_20'])\n",
    "if final_row_count != initial_row_count:\n",
    "    raise ValueError(f\"Row count mismatch: Expected {initial_row_count}, but got {final_row_count}\")\n",
    "else:\n",
    "    print(f\"Row count matches: {final_row_count} rows in the final dataframe.\")\n",
    "\n",
    "### 2. Select AMI limit based on household members\n",
    "# Round average household members to the nearest integer\n",
    "enhanced_df['hh_mem'] = enhanced_df['Average household members'].round().astype(int)\n",
    "\n",
    "# Select appropriate AMI limit based on household size\n",
    "enhanced_df['lim_income_120AMI'] = enhanced_df.apply(\n",
    "    lambda row: row['lim120_20p' + str(min(row['hh_mem'], 8))], axis=1\n",
    ")\n",
    "\n",
    "# Drop unnecessary AMI columns after selecting the relevant one\n",
    "enhanced_df.drop(columns=['lim120_20p1', 'lim120_20p2', 'lim120_20p3', 'lim120_20p4',\n",
    "                          'lim120_20p5', 'lim120_20p6', 'lim120_20p7', 'lim120_20p8'], inplace=True)\n",
    "\n",
    "### 3. Label homes as LMI (Low-to-Moderate Income) or not based on 120% AMI\n",
    "enhanced_df['LMI'] = np.where(enhanced_df['Average household income'] <= enhanced_df['lim_income_120AMI'], 1, 0)\n",
    "\n",
    "# Final check for row count consistency\n",
    "assert len(enhanced_df) == initial_row_count, \"Error: Row count mismatch after processing.\"\n",
    "print(f\" Final row count remains consistent: {len(enhanced_df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82907"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enhanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Add Census Divisions \n",
    "\n",
    "In RECS, this is the labelling \n",
    "\n",
    "-1 New England\n",
    "-2 Middle Atlantic\n",
    "-3 East North Central\n",
    "-4 West North Central\n",
    "-5 South Atlantic\n",
    "-6 East South Central\n",
    "-7 West South Central\n",
    "-8 Mountain North\n",
    "-9 Mountain South\n",
    "-10 Pacific\n",
    "\n",
    "https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf \n",
    "\n",
    "Note: there are 2 Mountain regions (N/S) - \"Mountain North\" (CO, ID, MT, UT, and WY) and \"Mountain South\" (AZ, NM, and NV) https://www.eia.gov/consumption/residential/reports/2009/16-states.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no puerto rico here\n",
    "\n",
    "state_to_census_region = {'01':6, '02':10, '04':9, '05':7, '06':10, '08':8, '09':1, '10':5,'11':5, '12':5,\n",
    "             '13':5, '15':10, '16':8, '17':3, '18':3, '19':4, '20':4, '21':6, '22':7,\n",
    "            '23':1, '24':5, '25':1, '26':3, '27':4, '28':6, '29':4, '30':8, '31':4,\n",
    "            '32':9, '33':1, '34':2, '35':9, '36':2, '37':5, '38':4, '39':3, '40':7,\n",
    "            '41':10, '42':2, '44':1, '45':5, '46':4, '47':6, '48':7, '49':8, '50':1,\n",
    "            '51':5, '53':10, '54':5, '55':3, '56':8}\n",
    "\n",
    "reg_to_region_dict = {\n",
    "    1: 'New England',\n",
    "    2: 'Middle Atlantic',\n",
    "    3: 'East North Central',\n",
    "    4: 'West North Central',\n",
    "    5: 'South Atlantic',\n",
    "    6: 'East South Central',\n",
    "    7: 'West South Central',\n",
    "    8: 'Mountain North',\n",
    "    9: 'Mountain South',\n",
    "    10: 'Pacific'\n",
    "}\n",
    "enhanced_df['DIVISION'] =  enhanced_df[\"state\"].map(state_to_census_region)\n",
    "enhanced_df['DIVISION'] =  enhanced_df[\"DIVISION\"].map(reg_to_region_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82907"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enhanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Add rural/urban\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows missing rural-urban data after merge: 0\n",
      "Row count is consistent after filling missing data: 82907 rows.\n"
     ]
    }
   ],
   "source": [
    "# Load the rural-urban population data from CSV\n",
    "urban_rural_df_raw = pd.read_csv(path + \"in_other_datasets/rural_urban/PctUrbanRural_County.csv\", sep=\";\")\n",
    "\n",
    "# Select relevant columns for analysis\n",
    "urban_rural_df = urban_rural_df_raw[['STATE', 'COUNTY', 'COUNTYNAME', 'STATENAME',\n",
    "                                     'POPPCT_RURAL', 'POPPCT_UC', 'POPPCT_UA',\n",
    "                                     'POP_RURAL', 'POP_UC', 'POP_UA']]\n",
    "\n",
    "# Create county-level GEOID by combining STATE and COUNTY codes\n",
    "urban_rural_df['STATE'] = urban_rural_df['STATE'].astype(str).str.zfill(2)\n",
    "urban_rural_df['COUNTY'] = urban_rural_df['COUNTY'].astype(str).str.zfill(3)\n",
    "urban_rural_df['county_GEOID_10'] = urban_rural_df['STATE'] + urban_rural_df['COUNTY']\n",
    "\n",
    "# Convert percentage columns to fractions (divide by 100)\n",
    "urban_rural_df[['frac_RURAL', 'frac_UC', 'frac_UA']] = urban_rural_df[['POPPCT_RURAL', 'POPPCT_UC', 'POPPCT_UA']].div(100)\n",
    "\n",
    "# Merge rural-urban fractions with enhanced_df\n",
    "enhanced_df = enhanced_df.merge(\n",
    "    urban_rural_df[['county_GEOID_10', 'frac_RURAL', 'frac_UC', 'frac_UA']],\n",
    "    how='left',  # Use 'left' merge to retain all rows in enhanced_df\n",
    "    left_on='county_GEOID_10',\n",
    "    right_on='county_GEOID_10'\n",
    ")\n",
    "\n",
    "# Check for missing values after merge\n",
    "missing_rural_data = enhanced_df['frac_RURAL'].isna().sum()\n",
    "print(f\"Number of rows missing rural-urban data after merge: {missing_rural_data}\")\n",
    "\n",
    "# If there are missing values, fill them with state-level averages\n",
    "if missing_rural_data > 0:\n",
    "    # Create state-level GEOID for enhanced_df and urban_rural_df\n",
    "    urban_rural_df['state_GEOID'] = urban_rural_df['county_GEOID_10'].str[:2]\n",
    "\n",
    "    # Calculate state-level averages for frac_RURAL, frac_UC, and frac_UA\n",
    "    state_avg = urban_rural_df.groupby('state_GEOID')[['frac_RURAL', 'frac_UC', 'frac_UA']].mean().reset_index()\n",
    "\n",
    "    # Fill missing values using state-level averages\n",
    "    for col in ['frac_RURAL', 'frac_UC', 'frac_UA']:\n",
    "        enhanced_df[col] = enhanced_df.apply(\n",
    "            lambda row: state_avg.loc[state_avg['state_GEOID'] == row['state_GEOID'], col].values[0]\n",
    "            if pd.isna(row[col]) and row['state_GEOID'] in state_avg['state_GEOID'].values\n",
    "            else row[col], axis=1\n",
    "        )\n",
    "\n",
    "    # Count the number of rows filled using state-level averages\n",
    "    remaining_missing_rows = enhanced_df['frac_RURAL'].isna().sum()\n",
    "    rows_filled_by_state = missing_rural_data - remaining_missing_rows\n",
    "    print(f\"Number of rows filled using state-level averages: {rows_filled_by_state}\")\n",
    "\n",
    "\n",
    "# Final consistency check\n",
    "if len(enhanced_df) != len(enhanced_df['GEOID_20']):\n",
    "    raise ValueError(f\"Row count mismatch: Expected {len(enhanced_df['GEOID_20'])}, but got {len(enhanced_df)}.\")\n",
    "else:\n",
    "    print(f\"Row count is consistent after filling missing data: {len(enhanced_df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82907"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enhanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Add energy prices per state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new 2020 energy prices form form 861\n",
    "#prices per utility\n",
    "e_prices = pd.read_excel(path+'in_other_datasets/energy_prices/Sales_Ult_Cust_2020.xlsx', skiprows = 2)\n",
    "e_prices.drop(e_prices.tail(1).index,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take away canadian pricesss from ME- Maine\n",
    "#https://www.eia.gov/electricity/gridmonitor/about\n",
    "condition = (e_prices['Utility Number'] == 1179) & (e_prices['BA Code'] =='NBSO')\n",
    "e_prices = e_prices[~condition] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9224932249322493\n"
     ]
    }
   ],
   "source": [
    "#only bundled service type: delivery + energy\n",
    "e_prices = e_prices[e_prices['Service Type'] == 'Bundled']\n",
    "\n",
    "e_prices_final = e_prices[['Utility Number', 'Utility Name', 'Service Type', \n",
    "                           'State', 'Thousand Dollars', 'Megawatthours', 'Count']]\n",
    "\n",
    "#re-insert nans \n",
    "e_prices_final = e_prices_final.replace('nan', np.nan)\n",
    "e_prices_final = e_prices_final.replace('', np.nan)\n",
    "e_prices_final = e_prices_final.replace('.', np.nan)\n",
    "e_prices_final = e_prices_final.replace(0, np.nan)\n",
    "    \n",
    "#Drop all nans in vb of interest              \n",
    "e_prices_final = e_prices_final.dropna(subset=['State', 'Thousand Dollars', 'Megawatthours' ], how='any')\n",
    "print(len(e_prices_final)/len(e_prices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#totals per state\n",
    "res_prices_state = e_prices_final.groupby('State').sum()\n",
    "\n",
    "#gen prices per state\n",
    "res_prices_state['elec_price_kwh'] = res_prices_state['Thousand Dollars'].div(res_prices_state['Megawatthours'])\n",
    "res_prices_state.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_fips_dict = {\n",
    "    'AL': '01', 'AK': '02', 'AZ': '04', 'AR': '05', 'CA': '06',\n",
    "    'CO': '08', 'CT': '09', 'DE': '10', 'DC': '11', 'FL': '12', 'GA': '13',\n",
    "    'HI': '15', 'ID': '16', 'IL': '17', 'IN': '18', 'IA': '19',\n",
    "    'KS': '20', 'KY': '21', 'LA': '22', 'ME': '23', 'MD': '24',\n",
    "    'MA': '25', 'MI': '26', 'MN': '27', 'MS': '28', 'MO': '29',\n",
    "    'MT': '30', 'NE': '31', 'NV': '32', 'NH': '33', 'NJ': '34',\n",
    "    'NM': '35', 'NY': '36', 'NC': '37', 'ND': '38', 'OH': '39',\n",
    "    'OK': '40', 'OR': '41', 'PA': '42', 'RI': '44', 'SC': '45',\n",
    "    'SD': '46', 'TN': '47', 'TX': '48', 'UT': '49', 'VT': '50',\n",
    "    'VA': '51', 'WA': '53', 'WV': '54', 'WI': '55', 'WY': '56'\n",
    "}\n",
    "\n",
    "# Map FIPS codes to a new column 'state'\n",
    "res_prices_state['state']  = res_prices_state['State'].map(state_fips_dict)\n",
    "len(res_prices_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82907\n"
     ]
    }
   ],
   "source": [
    "#merge prics\n",
    "enhanced_df = enhanced_df.merge(res_prices_state[['state','elec_price_kwh' ]], left_on = 'state' , right_on= 'state', how= 'inner')\n",
    "\n",
    "print(len(enhanced_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load energy prices and drop NaN values\n",
    "of_energy_prices = pd.read_excel(path + \"in_other_datasets/energy_prices/table_joule_e_prices.xlsx\", skiprows=1).dropna()\n",
    "\n",
    "# Create a DataFrame with new rows for Alaska (AK) and Hawaii (HI)\n",
    "new_rows = pd.DataFrame([\n",
    "    {'State': 'HI', 'NG ($/THERM)': 0, 'ELEC ($/KWH)': 0,\n",
    "     'FO ($/GAL)': of_energy_prices['FO ($/GAL)'].median(),\n",
    "     'PROPANE ($/GAL)': of_energy_prices['PROPANE ($/GAL)'].median()},\n",
    "    \n",
    "    {'State': 'AK', 'NG ($/THERM)': 0, 'ELEC ($/KWH)': 0,\n",
    "     'FO ($/GAL)': of_energy_prices['FO ($/GAL)'].median(),\n",
    "     'PROPANE ($/GAL)': of_energy_prices['PROPANE ($/GAL)'].median()}\n",
    "])\n",
    "\n",
    "# Use pd.concat to combine the original DataFrame and the new rows\n",
    "of_energy_prices = pd.concat([of_energy_prices, new_rows], ignore_index=True)\n",
    "\n",
    "# Map FIPS codes to a new column 'state' using the state_fips_dict\n",
    "of_energy_prices['state'] = of_energy_prices['State'].map(state_fips_dict)\n",
    "\n",
    "# Merge with enhanced_df to add fuel oil and propane costs\n",
    "enhanced_df = enhanced_df.merge(\n",
    "    of_energy_prices[['state', 'FO ($/GAL)', 'PROPANE ($/GAL)']], \n",
    "    on='state', \n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82907"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enhanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_gas = pd.read_excel(path +'in_other_datasets/energy_prices/gas_prices_state_eia.xlsx', skiprows = 2)\n",
    "\n",
    "# Dictionary mapping US state names to state abbreviations\n",
    "state_abbr = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n",
    "    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n",
    "    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n",
    "    'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
    "    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA',\n",
    "    'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD', 'Tennessee': 'TN',\n",
    "    'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA',\n",
    "    'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY', 'District of Columbia' : 'DC'\n",
    "}\n",
    "\n",
    "# Add a new column 'state_abbr' with state abbreviations\n",
    "prices_gas['state_abbr'] = prices_gas['Location'].map(state_abbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_gas_state = prices_gas[[2020,'state_abbr' ]]\n",
    "prices_gas_state.rename(columns = {2020:'gas_price_th_ft3'}, inplace = True)\n",
    "prices_gas_state = prices_gas_state[prices_gas_state.state_abbr.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82907"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge gas prices per state\n",
    "# Map FIPS codes to a new column 'state'\n",
    "prices_gas_state['state_fips']  = prices_gas_state['state_abbr'].map(state_fips_dict)\n",
    "enhanced_df = enhanced_df.merge(prices_gas_state[['state_fips', 'gas_price_th_ft3']],  left_on = 'state' , right_on= 'state_fips', how= 'inner')\n",
    "\n",
    "len(enhanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save other fuels\n",
    "enhanced_df['Fuel_other'] = enhanced_df['Propane (bottled gas)']+enhanced_df['Other_fuel'] +enhanced_df['Not applicable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop multiple columns from enhanced_df\n",
    "enhanced_df.drop(columns=['Unnamed: 0', 'state_GEOID'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Add climate zones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOIDs in enhanced_df before merge: 82907\n",
      "GEOIDs in df_intersect: 82683\n",
      "Number of missing GEOIDs after initial merge: 1140\n",
      "Finding nearest climate zones for 1140 unmatched tracts...\n",
      "Rows in enhanced_df after filling missing climates: 82907\n",
      "Unique GEOIDs after merge: 82907 (Should match original count)\n"
     ]
    }
   ],
   "source": [
    "# --- File Paths ---\n",
    "climate_zones_path = path + \"in_other_datasets/climate_zones/Climate_Zones_-_DOE_Building_America_Program.shp\"\n",
    "gis_ct_path = path + \"in_us_census/2020_gis_tracts/2020_us_tracts.shp\"\n",
    "\n",
    "# --- Load Climate Zones and Census Tract GIS Data ---\n",
    "# Read spatial files with EPSG:4326 CRS\n",
    "climate_zones = gpd.read_file(climate_zones_path).to_crs(epsg=4326)\n",
    "gis_ct = gpd.read_file(gis_ct_path).to_crs(epsg=4326)\n",
    "\n",
    "# --- Overlay to Find Intersections between Census Tracts and Climate Zones ---\n",
    "df_intersect = gpd.overlay(gis_ct, climate_zones, how='intersection')\n",
    "\n",
    "# Calculate area of overlap and keep the largest area for each tract\n",
    "df_intersect['area'] = df_intersect.geometry.area\n",
    "df_intersect = df_intersect.sort_values(by='area', ascending=False).drop_duplicates(subset='GEOID', keep='first')\n",
    "df_intersect.drop(columns=['area'], inplace=True)\n",
    "\n",
    "# --- Create IECC Climate Code Label ---\n",
    "df_intersect['IECC_climate_code'] = df_intersect['IECC_Clima'].astype(str) + df_intersect['IECC_Moist']\n",
    "\n",
    "# --- Add HAWAII Climate Zone 1A ---\n",
    "enhanced_df_HI = enhanced_df[enhanced_df['state'] == '15'].copy()\n",
    "enhanced_df_HI['IECC_climate_code'] = '1A'\n",
    "\n",
    "# Drop original Hawaii rows from enhanced_df\n",
    "enhanced_df_no_HI = enhanced_df[enhanced_df['state'] != '15']\n",
    "\n",
    "# Combine back with updated Hawaii rows\n",
    "enhanced_df = pd.concat([enhanced_df_no_HI, enhanced_df_HI], ignore_index=True)\n",
    "\n",
    "# --- Add ALASKA Climate Zones (Zone 7 and Zone 8) ---\n",
    "# Mapping from location to county and FIPS\n",
    "location_to_county = {\n",
    "    'Aleutians East Borough': 'Aleutians East',\n",
    "    'Aleutians West Census Area': 'Aleutians West',\n",
    "    'Southeast Fairbanks Census Area': 'Southeast Fairbanks',\n",
    "    'Dillingham Census Area': 'Dillingham',\n",
    "    'North Slope Borough': 'North Slope',\n",
    "    'Yakutat plus Hoonah-Angoon': 'Hoonah-Angoon',\n",
    "    'Kodiak Island Borough': 'Kodiak Island',\n",
    "    'Skagway Municipality': 'Skagway-Hoonah-Angoon',\n",
    "    'Northwest Arctic Borough': 'Northwest Arctic',\n",
    "    'Ketchikan Gateway Borough': 'Ketchikan Gateway',\n",
    "    'Kusilvak Census Area': 'Kusilvak',\n",
    "    'Fairbanks North Star Borough': 'Fairbanks North Star',\n",
    "    'Juneau City and Borough': 'Juneau',\n",
    "    'Nome Census Area': 'Nome',\n",
    "    'Prince of Wales-Hyder Census Area': 'Prince of Wales-Hyder',\n",
    "    'Sitka City and Borough': 'Sitka',\n",
    "    'Petersburg Borough': 'Petersburg',\n",
    "    'Wrangell City and Borough': 'Wrangell',\n",
    "    'Matanuska-Susitna Borough': 'Matanuska-Susitna',\n",
    "    'Yukon-Koyukuk Census Area': 'Yukon-Koyukuk',\n",
    "    'Kenai Peninsula Borough': 'Kenai Peninsula',\n",
    "    'Haines Borough': 'Haines',\n",
    "    'Bristol Bay plus Lake and Peninsula': 'Lake and Peninsula',\n",
    "    'Bethel Census Area': 'Bethel',\n",
    "    'Denali Borough': 'Denali',\n",
    "    'Anchorage Municipality': 'Anchorage',\n",
    "    'Valdez-Cordova Census Area': 'Valdez-Cordova'\n",
    "}\n",
    "\n",
    "county_to_fips = {\n",
    "    'Aleutians East': '013',\n",
    "    'Aleutians West': '016',\n",
    "    'Southeast Fairbanks': '240',\n",
    "    'Dillingham': '070',\n",
    "    'North Slope': '185',\n",
    "    'Hoonah-Angoon': '105',\n",
    "    'Kodiak Island': '150',\n",
    "    'Skagway-Hoonah-Angoon': '230',\n",
    "    'Northwest Arctic': '188',\n",
    "    'Ketchikan Gateway': '130',\n",
    "    'Kusilvak': '158',\n",
    "    'Fairbanks North Star': '090',\n",
    "    'Juneau': '110',\n",
    "    'Nome': '180',\n",
    "    'Prince of Wales-Hyder': '201',\n",
    "    'Sitka': '220',\n",
    "    'Petersburg': '195',\n",
    "    'Wrangell': '275',\n",
    "    'Matanuska-Susitna': '170',\n",
    "    'Yukon-Koyukuk': '290',\n",
    "    'Kenai Peninsula': '122',\n",
    "    'Haines': '100',\n",
    "    'Lake and Peninsula': '164',\n",
    "    'Bethel': '050',\n",
    "    'Denali': '068',\n",
    "    'Anchorage': '020',\n",
    "    'Valdez-Cordova': '261'\n",
    "}\n",
    "\n",
    "# Boroughs in Zone 8 (special cases)\n",
    "boroughs_zone_8 = [\n",
    "    'Bethel Census Area', 'Northwest Arctic Borough', 'Dillingham Census Area',\n",
    "    'Southeast Fairbanks Census Area', 'Fairbanks North Star Borough',\n",
    "    'North Slope Borough', 'Kusilvak Census Area', 'Nome Census Area',\n",
    "    'Yukon-Koyukuk Census Area'\n",
    "]\n",
    "\n",
    "# Get FIPS codes for Zone 8 Boroughs\n",
    "fips_8AK = [county_to_fips[location_to_county[borough]] for borough in boroughs_zone_8]\n",
    "\n",
    "# Assign climate zones to Alaska rows based on county FIPS\n",
    "enhanced_df.loc[(enhanced_df['county'].isin(fips_8AK)) & (enhanced_df['state'] == '02'), 'IECC_climate_code'] = '8AK'\n",
    "enhanced_df.loc[(~enhanced_df['county'].isin(fips_8AK)) & (enhanced_df['state'] == '02'), 'IECC_climate_code'] = '7AK'\n",
    "\n",
    "# --- Initial Merge with Climate Zones ---\n",
    "print(f\"GEOIDs in enhanced_df before merge: {enhanced_df['GEOID_20'].nunique()}\")\n",
    "print(f\"GEOIDs in df_intersect: {df_intersect['GEOID'].nunique()}\")\n",
    "\n",
    "# Merge and avoid duplicates using suffixes\n",
    "enhanced_df = enhanced_df.merge(df_intersect[['GEOID', 'IECC_climate_code']],\n",
    "                                 left_on='GEOID_20', right_on='GEOID', \n",
    "                                 how='left', suffixes=('', '_new'))\n",
    "\n",
    "# --- Fix Duplicate Columns ---\n",
    "# If IECC_climate_code already exists, fill NaN values with new data\n",
    "if 'IECC_climate_code_new' in enhanced_df.columns:\n",
    "    enhanced_df['IECC_climate_code'] = enhanced_df['IECC_climate_code'].combine_first(enhanced_df['IECC_climate_code_new'])\n",
    "    # Drop the extra column after merging\n",
    "    enhanced_df.drop(columns=['IECC_climate_code_new', 'GEOID'], inplace=True)\n",
    "\n",
    "# --- Check for Missing GEOIDs Only After Initial Merge ---\n",
    "missing_geoids = enhanced_df[enhanced_df['IECC_climate_code'].isnull()]\n",
    "print(f\"Number of missing GEOIDs after initial merge: {len(missing_geoids)}\")\n",
    "\n",
    "# --- Handle Missing GEOIDs Using Nearest Neighbor ---\n",
    "if len(missing_geoids) > 0:\n",
    "    print(f\"Finding nearest climate zones for {len(missing_geoids)} unmatched tracts...\")\n",
    "\n",
    "    # Get unmatched census tracts from GIS file\n",
    "    unmatched_geoids = gis_ct[gis_ct['GEOID'].isin(missing_geoids['GEOID_20'])].copy()\n",
    "\n",
    "    # Precompute nearest climate zone for unmatched tracts\n",
    "    nearest_climate_codes = []\n",
    "    for index, row in unmatched_geoids.iterrows():\n",
    "        point = row.geometry\n",
    "        nearest_zone_index = climate_zones.distance(point).idxmin()\n",
    "        nearest_zone = climate_zones.iloc[nearest_zone_index]\n",
    "\n",
    "        # Create the climate code for the nearest zone\n",
    "        nearest_climate_codes.append({\n",
    "            'GEOID_20': row['GEOID'],\n",
    "            'IECC_climate_code': str(nearest_zone['IECC_Clima']) + str(nearest_zone['IECC_Moist'])\n",
    "        })\n",
    "\n",
    "    # Create DataFrame with nearest climate matches\n",
    "    nearest_climate_df = pd.DataFrame(nearest_climate_codes)\n",
    "\n",
    "    # Merge nearest climate zones with unmatched rows\n",
    "    enhanced_df = enhanced_df.merge(nearest_climate_df, on='GEOID_20', how='left', suffixes=('', '_nearest'))\n",
    "\n",
    "    # Fill missing values from nearest match\n",
    "    enhanced_df['IECC_climate_code'] = enhanced_df['IECC_climate_code'].combine_first(enhanced_df['IECC_climate_code_nearest'])\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    enhanced_df.drop(columns=['IECC_climate_code_nearest'], inplace=True)\n",
    "\n",
    "# --- Final Sanity Checks ---\n",
    "# Ensure that all original rows are retained and no duplicates are introduced\n",
    "print(f\"Rows in enhanced_df after filling missing climates: {len(enhanced_df)}\")\n",
    "print(f\"Unique GEOIDs after merge: {enhanced_df['GEOID_20'].nunique()} (Should match original count)\")\n",
    "\n",
    "# Fill any remaining NaN climate codes with a placeholder if necessary\n",
    "enhanced_df['IECC_climate_code'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82907"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enhanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of CTs saved from contiguous US, 2020: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Fraction of CTs saved from contiguous US, 2020:', len(enhanced_df)/len(census_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of CTs saved from all US, 2020: 0.9693550650079507\n"
     ]
    }
   ],
   "source": [
    "print('Fraction of CTs saved from all US, 2020:', len(enhanced_df)/len(gis_ct)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Save all raw variables per census tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all vaiables for later analysis \n",
    "enhanced_df.to_csv(path+'out_final/ct_enhanced_analysis_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Match RECS X variable structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only variables needed to apply models to\n",
    "df_vars = enhanced_df[[ 'county', 'tract', 'CDD', 'HDD', 'Average year built','Average num bedrooms', 'Average num rooms',\n",
    "                           'Average household members', 'Average household income', 'cost_EL', 'cost_NG','cost_OF',\n",
    "                           '5 or more units', 'Mobile home', 'Attached, 1 unit', 'Detached, 1 unit','IECC_climate_code',\n",
    "                           'Renter occupied','Electricity', 'Fuel oil', 'Natural gas from underground pipes', 'Fuel_other',\n",
    "                           'Wood or pellets', 'DIVISION', 'state', 'frac_RURAL', 'frac_UA', 'frac_UC', 'total_population']]\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dummies if needed\n",
    "df_vars = pd.concat([df_vars, pd.get_dummies(pd.Series(df_vars.state), drop_first=False, prefix='STATE_FIPS')] , axis=1)\n",
    "df_vars = pd.concat([df_vars, pd.get_dummies(pd.Series(df_vars.IECC_climate_code), drop_first=False, prefix='IECC_climate_code')] , axis=1)\n",
    "df_vars = pd.concat([df_vars, pd.get_dummies(pd.Series(df_vars.DIVISION), drop_first=False, prefix='Census Div')] , axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are some climate zones and states represented in the RECS that we dont havein th acs \n",
    "df_vars['IECC_climate_code_7A'] = np.zeros(len(df_vars))\n",
    "df_vars['IECC_climate_code_7B']= np.zeros(len(df_vars))\n",
    "\n",
    "#put in value for CO and WY that had zone z of unknown humidiy level\n",
    "#7A - 27, 38, 55, 23, 26 \n",
    "#7B - 8, 56\n",
    "df_vars.loc[(df_vars['IECC_climate_code_7N/A'] == 1) & (df_vars['state'].isin(['27', '38', '55', '23', '26' ])), 'IECC_climate_code_7A'] = 1\n",
    "df_vars.loc[(df_vars['IECC_climate_code_7N/A'] == 1) & (df_vars['state'].isin(['08', '56' ])), 'IECC_climate_code_7B'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummies that have been dropped\n",
    "#'2 to 4 units',\n",
    "# 'Owner occupied',\n",
    "# East North Central \n",
    "#'frac_UC',\n",
    "#'IECC_climate_code_1A',\n",
    "#'STATE_FIPS_01',\n",
    "#drop the IECC_climate_code_7N/A and IECC_climate_code_8N/A categories\n",
    "\n",
    "df_vars_clean = df_vars[['state','county', 'tract', 'CDD', 'HDD', 'Average year built',\n",
    "       'Average num bedrooms', 'Average num rooms',\n",
    "       'Average household members', 'Average household income', 'cost_EL',\n",
    "       'cost_NG', 'cost_OF', '5 or more units', 'Mobile home',\n",
    "       'Attached, 1 unit', 'Detached, 1 unit', 'IECC_climate_code_2A','IECC_climate_code_2B', \n",
    "       'IECC_climate_code_3A','IECC_climate_code_3B', 'IECC_climate_code_3C', 'IECC_climate_code_4A',\n",
    "       'IECC_climate_code_4B', 'IECC_climate_code_4C', 'IECC_climate_code_5A',\n",
    "       'IECC_climate_code_5B', 'IECC_climate_code_6A', 'IECC_climate_code_6B',\n",
    "       'IECC_climate_code_7A', 'IECC_climate_code_7AK', 'IECC_climate_code_7B', 'IECC_climate_code_8AK',\n",
    "       'Renter occupied', 'Electricity', 'Fuel oil',\n",
    "       'Natural gas from underground pipes', 'Fuel_other', 'Wood or pellets',\n",
    "       'Census Div_East South Central', 'Census Div_Middle Atlantic',\n",
    "       'Census Div_Mountain North', 'Census Div_Mountain South',\n",
    "       'Census Div_New England', 'Census Div_Pacific',\n",
    "       'Census Div_South Atlantic', 'Census Div_West North Central',\n",
    "       'Census Div_West South Central','STATE_FIPS_02','STATE_FIPS_04',\n",
    "       'STATE_FIPS_05','STATE_FIPS_06','STATE_FIPS_08','STATE_FIPS_09',\n",
    "       'STATE_FIPS_10','STATE_FIPS_11','STATE_FIPS_12','STATE_FIPS_13',\n",
    "       'STATE_FIPS_15', 'STATE_FIPS_16','STATE_FIPS_17','STATE_FIPS_18',\n",
    "       'STATE_FIPS_19','STATE_FIPS_20','STATE_FIPS_21','STATE_FIPS_22',\n",
    "       'STATE_FIPS_23','STATE_FIPS_24','STATE_FIPS_25','STATE_FIPS_26',\n",
    "       'STATE_FIPS_27','STATE_FIPS_28','STATE_FIPS_29','STATE_FIPS_30',\n",
    "       'STATE_FIPS_31','STATE_FIPS_32','STATE_FIPS_33','STATE_FIPS_34',\n",
    "       'STATE_FIPS_35', 'STATE_FIPS_36','STATE_FIPS_37','STATE_FIPS_38',\n",
    "       'STATE_FIPS_39','STATE_FIPS_40','STATE_FIPS_41','STATE_FIPS_42',\n",
    "       'STATE_FIPS_44','STATE_FIPS_45','STATE_FIPS_46','STATE_FIPS_47',\n",
    "       'STATE_FIPS_48','STATE_FIPS_49','STATE_FIPS_50','STATE_FIPS_51',\n",
    "       'STATE_FIPS_53','STATE_FIPS_54','STATE_FIPS_55','STATE_FIPS_56','frac_RURAL', 'frac_UC',\n",
    "        'total_population']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reclassify numeric variables\n",
    "\n",
    "- Average year built\n",
    "- Average num bedrooms\n",
    "- Average num rooms\n",
    "- Average household income\n",
    "- Average household members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Year built\n",
    "# Define the bin edges and labels\n",
    "bins = [1945, 1950, 1960, 1970, 1980, 1990, 2000, 2010, float('inf')]\n",
    "labels = [1945, 1955, 1965, 1975, 1985, 1995, 2005, 2015]\n",
    "\n",
    "# Reclassify the values in the 'Year' column based on the conditions\n",
    "df_vars_clean['Average year built'] = pd.cut(df_vars_clean['Average year built'], bins=bins, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bedrooms\n",
    "# Define the bin edges and labels\n",
    "bins = [0, 0.5, 1.5, 2.5, 3.5, 4.5, float('inf')]\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Reclassify the values in the 'Year' column based on the conditions\n",
    "df_vars_clean['Average num bedrooms'] = pd.cut(df_vars_clean['Average num bedrooms'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rooms\n",
    "# Define the bin edges and labels\n",
    "bins = [0, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, float('inf')]\n",
    "labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Reclassify the values in the 'Year' column based on the conditions\n",
    "df_vars_clean['Average num rooms'] = pd.cut(df_vars_clean['Average num rooms'], bins=bins, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HH members\n",
    "# Define the bin edges and labels\n",
    "bins = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, float('inf')]\n",
    "labels = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Reclassify the values in the 'Year' column based on the conditions\n",
    "df_vars_clean['Average household members'] = pd.cut(df_vars_clean['Average household members'], bins=bins, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Income\n",
    "# Define the bin edges and labels\n",
    "bins = [0, 10000, 15000, 20000, 25000, 30000, 35000, 40000, 50000, 60000, 75000, 100000, 150000, float('inf')]\n",
    "labels = [5000, 12500, 17500, 22500, 27500, 32500, 37500, 45500, 55500, 67500, 87500, 125000, 175000]\n",
    "\n",
    "# Reclassify the values in the 'Year' column based on the conditions\n",
    "df_vars_clean['Average household income'] = pd.cut(df_vars_clean['Average household income'], bins=bins, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data type of the columns from 'category' to 'float'\n",
    "df_vars_clean[['Average year built',\n",
    "       'Average num bedrooms', 'Average num rooms', 'Average household income',\n",
    "       'Average household members']] = df_vars_clean[['Average year built',\n",
    "       'Average num bedrooms', 'Average num rooms', 'Average household income',\n",
    "       'Average household members']].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Normalize ACS variables using RECS mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling VBS - get the average and std of the RECS vbs\n",
    "RECS_raw = pd.read_csv(path + 'RECS/recs2020_public_v5.csv')\n",
    "\n",
    "#get normalization parameters\n",
    "X_sub, Y, RECS_norm_param = recs_EDA.vb_transform(RECS_raw) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary for renaming index values\n",
    "index_rename_dict = {'HDD30YR_PUB': 'HDD', 'CDD30YR_PUB':'CDD', 'DOLLAREL':'cost_EL', 'DOLLARNG':'cost_NG', 'DOLLAROF':'cost_OF','YEARMADERANGE':'Average year built', \n",
    "                      'BEDROOMS':'Average num bedrooms', 'TOTROOMS':'Average num rooms', 'MONEYPY':'Average household income', 'NHSLDMEM':'Average household members'}\n",
    "\n",
    "# Rename index values using the dictionary\n",
    "RECS_norm_param = RECS_norm_param.rename(index=index_rename_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code to normalize recs and ACS data\n",
    "cols_to_norm = ['HDD', 'CDD', 'cost_EL', 'cost_NG', 'cost_OF', 'Average year built',\n",
    "       'Average num bedrooms', 'Average num rooms', 'Average household income',\n",
    "       'Average household members']\n",
    "\n",
    "# Normalize the selected columns in the original DataFrame (df)\n",
    "df_vars_clean[cols_to_norm] = (df_vars_clean[cols_to_norm] - RECS_norm_param['Mean']) / RECS_norm_param['std']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Save outputs for model application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe \n",
    "df_vars_clean.to_csv(path+'out_final/ct_enhanced_model_application_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
